from dotenv import load_dotenv

load_dotenv()

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
import os

# --- å°‚é–€å®¶ã®å½¹å‰²å®šç¾© ---
expert_roles = {
    "æ³•å¾‹ã®å°‚é–€å®¶": "ã‚ãªãŸã¯æ³•å¾‹ã®å°‚é–€å®¶ã§ã™ã€‚æ³•å¾‹ã«é–¢ã™ã‚‹è³ªå•ã«å¯¾ã—ã¦ã€ã‚ã‹ã‚Šã‚„ã™ãä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
    "åŒ»ç™‚ã®å°‚é–€å®¶": "ã‚ãªãŸã¯åŒ»ç™‚ã®å°‚é–€å®¶ã§ã™ã€‚å¥åº·ã‚„ç—…æ°—ã«é–¢ã™ã‚‹è³ªå•ã«ã€å°‚é–€çš„ã‹ã¤ã‚„ã•ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    "é‡‘èã®å°‚é–€å®¶": "ã‚ãªãŸã¯é‡‘èã®å°‚é–€å®¶ã§ã™ã€‚ãŠé‡‘ã‚„æŠ•è³‡ã«é–¢ã™ã‚‹è³ªå•ã«ã€æ­£ç¢ºã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„ã€‚"
}

# --- LLM å‘¼ã³å‡ºã—é–¢æ•° ---
def ask_expert(question: str, role: str) -> str:
    system_prompt = expert_roles.get(role, "ã‚ãªãŸã¯æœ‰èƒ½ãªå°‚é–€å®¶ã§ã™ã€‚")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question)
    ]

    result = llm.invoke(messages)
    return result.content

# --- Streamlit UI ---
st.title("ğŸ§  å°‚é–€å®¶ã«ç›¸è«‡ã§ãã‚‹AIãƒãƒ£ãƒƒãƒˆ")

st.write("##### ä½¿ã„æ–¹")
st.write("- å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸ã‚“ã§ã€ç›¸è«‡ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
st.write("- ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€AIãŒå°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¾ã™ã€‚")

# --- UIå…¥åŠ› ---
selected_expert = st.radio("å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š", list(expert_roles.keys()))
question = st.text_area("ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", height=150)

# --- å®Ÿè¡Œ ---
if st.button("é€ä¿¡"):
    if not question.strip():
        st.error("è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("AIãŒè€ƒãˆä¸­ã§ã™..."):
            response = ask_expert(question, selected_expert)
            st.success("âœ… å›ç­”:")
            st.write(response)